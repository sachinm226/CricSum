{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq8zshYaH6FV",
        "outputId": "6c033255-4946-49c2-a2a3-3bdad9c93eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Dec 13 13:32:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to5smjBphRf0",
        "outputId": "e45ca072-17b5-4579-8ae0-3071b2a71c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7WE3KuQuItR",
        "outputId": "25b50b45-947a-48b2-afb2-709944f3a5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWeX8exGvAd5",
        "outputId": "1114fc4c-ab53-49da-e589-8d2e1d827da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.5.5)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2021.11.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.8)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pFmd1bzyyGs",
        "outputId": "0307e2a7-2e0d-4344-c9f8-ff1c8aa79e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCf40j5VtLpo",
        "outputId": "7cd3f27c-a8d2-410d-87fd-a3354593af44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    MT5ForConditionalGeneration,\n",
        "    ByT5Tokenizer,\n",
        "    PreTrainedTokenizer,\n",
        "    T5TokenizerFast as T5Tokenizer,\n",
        "    MT5TokenizerFast as MT5Tokenizer,\n",
        ")\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# from fastT5 import export_and_get_onnx_model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "pl.seed_everything(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2I5fD595ueVR"
      },
      "outputs": [],
      "source": [
        "class PyTorchDataModule(Dataset):\n",
        "    \"\"\"  Dataset class  \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        source_max_token_len: int = 512,\n",
        "        target_max_token_len: int = 512,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a PyTorch Dataset Module for input data\n",
        "        Args:\n",
        "            data (pd.DataFrame): input pandas dataframe. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
        "            tokenizer (PreTrainedTokenizer): a PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n",
        "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
        "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.source_max_token_len = source_max_token_len\n",
        "        self.target_max_token_len = target_max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" returns length of data \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\" returns dictionary of input tensors to feed into T5 model\"\"\"\n",
        "\n",
        "        data_row = self.data.iloc[index]\n",
        "        source_text = data_row[\"source_text\"]\n",
        "\n",
        "        source_text_encoding = self.tokenizer(\n",
        "            source_text,\n",
        "            max_length=self.source_max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        target_text_encoding = self.tokenizer(\n",
        "            data_row[\"target_text\"],\n",
        "            max_length=self.target_max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels = target_text_encoding[\"input_ids\"]\n",
        "        labels[\n",
        "            labels == 0\n",
        "        ] = -100  # to make sure we have correct labels for T5 text generation\n",
        "\n",
        "        return dict(\n",
        "            source_text=source_text,\n",
        "            target_text=data_row[\"target_text\"],\n",
        "            source_text_input_ids=source_text_encoding[\"input_ids\"].flatten(),\n",
        "            source_text_attention_mask=source_text_encoding[\"attention_mask\"].flatten(),\n",
        "            labels=labels.flatten(),\n",
        "            labels_attention_mask=target_text_encoding[\"attention_mask\"].flatten(),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HT3YTCVIvO1w"
      },
      "outputs": [],
      "source": [
        "class LightningDataModule(pl.LightningDataModule):\n",
        "    \"\"\" Lightning data class \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        test_df: pd.DataFrame,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        batch_size: int = 4,\n",
        "        source_max_token_len: int = 512,\n",
        "        target_max_token_len: int = 512,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a PyTorch Lightning Data Module\n",
        "        Args:\n",
        "            train_df (pd.DataFrame): training dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n",
        "            test_df (pd.DataFrame): validation dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n",
        "            tokenizer (PreTrainedTokenizer): PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n",
        "            batch_size (int, optional): batch size. Defaults to 4.\n",
        "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
        "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.source_max_token_len = source_max_token_len\n",
        "        self.target_max_token_len = target_max_token_len\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = PyTorchDataModule(\n",
        "            self.train_df,\n",
        "            self.tokenizer,\n",
        "            self.source_max_token_len,\n",
        "            self.target_max_token_len,\n",
        "        )\n",
        "        self.test_dataset = PyTorchDataModule(\n",
        "            self.test_df,\n",
        "            self.tokenizer,\n",
        "            self.source_max_token_len,\n",
        "            self.target_max_token_len,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\" training dataloader \"\"\"\n",
        "        return DataLoader(\n",
        "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\" test dataloader \"\"\"\n",
        "        return DataLoader(\n",
        "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\" validation dataloader \"\"\"\n",
        "        return DataLoader(\n",
        "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dmh38OKqvR0O"
      },
      "outputs": [],
      "source": [
        "class LightningModel(pl.LightningModule):\n",
        "    \"\"\" Lightning Model class\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, model, outputdir: str = \"outputs\"):\n",
        "        \"\"\"\n",
        "        initCreatesiates a PyTorch Lightning Model\n",
        "        Args:\n",
        "            tokenizer : T5 tokenizer\n",
        "            model : T5 model\n",
        "            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.outputdir = outputdir\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
        "        \"\"\" forward step \"\"\"\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_size):\n",
        "        input_ids = batch[\"source_text_input_ids\"]\n",
        "        attention_mask = batch[\"source_text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=labels_attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_size):\n",
        "        input_ids = batch[\"source_text_input_ids\"]\n",
        "        attention_mask = batch[\"source_text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=labels_attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_size):\n",
        "        input_ids = batch[\"source_text_input_ids\"]\n",
        "        attention_mask = batch[\"source_text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=labels_attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\" optimizers creation \"\"\"\n",
        "        return AdamW(self.parameters(), lr=0.0001)\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        \"\"\" save tokenizer and model on epoch end \"\"\"\n",
        "        avg_traning_loss = np.round(\n",
        "            torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])).item(),\n",
        "            4,\n",
        "        )\n",
        "        path = f\"{self.outputdir}/simplet5-epoch-{self.current_epoch}-train-loss-{str(avg_traning_loss)}\"\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "        self.model.save_pretrained(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_srXrTxdvfju"
      },
      "outputs": [],
      "source": [
        "class T5Sum:\n",
        "    \"\"\" Custom T5Sum class \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\" Creates T5Sum class \"\"\"\n",
        "        pass\n",
        "\n",
        "    def from_pretrained(self, model_type=\"t5\", model_name=\"t5-base\") -> None:\n",
        "        \"\"\"\n",
        "        loads T5 Model model for training/finetuning\n",
        "        Args:\n",
        "            model_type (str, optional): \"t5\" or \"mt5\" . Defaults to \"t5\".\n",
        "            model_name (str, optional): exact model architecture name, \"t5-base\" or \"t5-large\". Defaults to \"t5-base\".\n",
        "        \"\"\"\n",
        "        if model_type == \"t5\":\n",
        "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_name}\")\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
        "                f\"{model_name}\", return_dict=True\n",
        "            )\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        eval_df: pd.DataFrame,\n",
        "        source_max_token_len: int = 512,\n",
        "        target_max_token_len: int = 512,\n",
        "        batch_size: int = 8,\n",
        "        max_epochs: int = 5,\n",
        "        use_gpu: bool = True,\n",
        "        outputdir: str = \"outputs\",\n",
        "        early_stopping_patience_epochs: int = 0,  # 0 to disable early stopping feature\n",
        "        precision=32,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        trains T5 model on custom dataset\n",
        "        Args:\n",
        "            train_df (pd.DataFrame): training datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
        "            eval_df ([type], optional): validation datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
        "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
        "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
        "            batch_size (int, optional): batch size. Defaults to 8.\n",
        "            max_epochs (int, optional): max number of epochs. Defaults to 5.\n",
        "            use_gpu (bool, optional): if True, model uses gpu for training. Defaults to True.\n",
        "            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n",
        "            early_stopping_patience_epochs (int, optional): monitors val_loss on epoch end and stops training, if val_loss does not improve after the specied number of epochs. set 0 to disable early stopping. Defaults to 0 (disabled)\n",
        "            precision (int, optional): sets precision training - Double precision (64), full precision (32) or half precision (16). Defaults to 32.\n",
        "        \"\"\"\n",
        "        self.target_max_token_len = target_max_token_len\n",
        "        self.data_module = LightningDataModule(\n",
        "            train_df,\n",
        "            eval_df,\n",
        "            self.tokenizer,\n",
        "            batch_size=batch_size,\n",
        "            source_max_token_len=source_max_token_len,\n",
        "            target_max_token_len=target_max_token_len,\n",
        "        )\n",
        "\n",
        "        self.T5Model = LightningModel(\n",
        "            tokenizer=self.tokenizer, model=self.model, outputdir=outputdir\n",
        "        )\n",
        "\n",
        "        early_stop_callback = (\n",
        "            [\n",
        "                EarlyStopping(\n",
        "                    monitor=\"val_loss\",\n",
        "                    min_delta=0.00,\n",
        "                    patience=early_stopping_patience_epochs,\n",
        "                    verbose=True,\n",
        "                    mode=\"min\",\n",
        "                )\n",
        "            ]\n",
        "            if early_stopping_patience_epochs > 0\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        gpus = 1 if use_gpu else 0\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            # logger=logger,\n",
        "            callbacks=early_stop_callback,\n",
        "            max_epochs=max_epochs,\n",
        "            gpus=gpus,\n",
        "            progress_bar_refresh_rate=5,\n",
        "            precision=precision,\n",
        "        )\n",
        "\n",
        "        trainer.fit(self.T5Model, self.data_module)\n",
        "\n",
        "    def load_model(\n",
        "        self, model_type: str = \"t5\", model_dir: str = \"outputs\", use_gpu: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        loads a checkpoint for inferencing/prediction\n",
        "        Args:\n",
        "            model_type (str, optional): \"t5\" or \"mt5\". Defaults to \"t5\".\n",
        "            model_dir (str, optional): path to model directory. Defaults to \"outputs\".\n",
        "            use_gpu (bool, optional): if True, model uses gpu for inferencing/prediction. Defaults to True.\n",
        "        \"\"\"\n",
        "        if model_type == \"t5\":\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
        "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
        "\n",
        "        if use_gpu:\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda\")\n",
        "            else:\n",
        "                raise \n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        source_text: str,\n",
        "        max_length: int = 512,\n",
        "        num_return_sequences: int = 1,\n",
        "        num_beams: int = 2,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.95,\n",
        "        do_sample: bool = True,\n",
        "        repetition_penalty: float = 2.5,\n",
        "        length_penalty: float = 1.0,\n",
        "        early_stopping: bool = True,\n",
        "        skip_special_tokens: bool = True,\n",
        "        clean_up_tokenization_spaces: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        generates prediction for T5 model\n",
        "        Args:\n",
        "            source_text (str): any text for generating predictions\n",
        "            max_length (int, optional): max token length of prediction. Defaults to 512.\n",
        "            num_return_sequences (int, optional): number of predictions to be returned. Defaults to 1.\n",
        "            num_beams (int, optional): number of beams. Defaults to 2.\n",
        "            top_k (int, optional): Defaults to 50.\n",
        "            top_p (float, optional): Defaults to 0.95.\n",
        "            do_sample (bool, optional): Defaults to True.\n",
        "            repetition_penalty (float, optional): Defaults to 2.5.\n",
        "            length_penalty (float, optional): Defaults to 1.0.\n",
        "            early_stopping (bool, optional): Defaults to True.\n",
        "            skip_special_tokens (bool, optional): Defaults to True.\n",
        "            clean_up_tokenization_spaces (bool, optional): Defaults to True.\n",
        "        Returns:\n",
        "            list[str]: returns predictions\n",
        "        \"\"\"\n",
        "        input_ids = self.tokenizer.encode(\n",
        "            source_text, return_tensors=\"pt\", add_special_tokens=True\n",
        "        )\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            num_beams=num_beams,\n",
        "            max_length=max_length,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            length_penalty=length_penalty,\n",
        "            early_stopping=early_stopping,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "        preds = [\n",
        "            self.tokenizer.decode(\n",
        "                g,\n",
        "                skip_special_tokens=skip_special_tokens,\n",
        "                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
        "            )\n",
        "            for g in generated_ids\n",
        "        ]\n",
        "        return preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DwZPvlNuLtIc"
      },
      "outputs": [],
      "source": [
        "# Fetch data into daataframe\n",
        "data_frame = pd.read_excel(r'/content/drive/MyDrive/Data/data_2_withoutMask.xlsx')\n",
        "data_frame.drop('Unnamed: 0', axis =1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ueqm9-TuxyBN"
      },
      "outputs": [],
      "source": [
        "# Data spliting\n",
        "train_df = data_frame.iloc[:372, :]\n",
        "eval_df = data_frame.iloc[372:(372+47), :]\n",
        "test_df = data_frame.iloc[(372+47):,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cXOaMR3Ix7S-"
      },
      "outputs": [],
      "source": [
        "# instantiate\n",
        "model = T5Sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YSY3rToU_HJf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lBduYngV_5Hg"
      },
      "outputs": [],
      "source": [
        "model.from_pretrained(\"t5\",\"t5-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fce76502e0e94222a452690bc8b90c61",
            "86ef577dcccd43d4b5b308716fd6399a",
            "5cb6661963644bf082366b960385a7e1",
            "b73f8c274aae4f57b657521f5a9fab90",
            "d75db3dd436740e9b4670e5383fdf189",
            "a05750c190784f798c422bad583e0515",
            "38464ca0807c49388d06aa8860bce6a7",
            "ba52d3fa74c148d596417390e9ba4ac7",
            "57e9672b3efa4818ab1ac8cd2a67384a",
            "8856f77313c24a22925e551e6794a75e",
            "47afb74e6fbc48d6a77b2c031cbb6b75",
            "d1943cb5b04d4e779b0d7421414b0688",
            "de712ab9bdd741668d301aec55be8d6e",
            "40114a9cfc964ed1aff3a609639566bd",
            "1e53b8f84aac43c398639381bf71ec10",
            "a3a0ae8bf8794bb8a27dc4a1bacaba81",
            "3db3e63ccd8f47739ab6f5371b64e9e2",
            "d5099f6256a040d4aa7c9465f4870203",
            "ea9df24dbb354845a752152ad4f4e64f",
            "e51e9781a6ce48c69ec832ba7ee86a5c",
            "b72016de2276448686c346fc9991e580"
          ]
        },
        "id": "Ka8nSQptxyR0",
        "outputId": "4306d5d0-8332-4146-8ff8-fbae4472518f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fce76502e0e94222a452690bc8b90c61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1943cb5b04d4e779b0d7421414b0688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de712ab9bdd741668d301aec55be8d6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40114a9cfc964ed1aff3a609639566bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=5)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 222 M \n",
            "-----------------------------------------------------\n",
            "222 M     Trainable params\n",
            "0         Non-trainable params\n",
            "222 M     Total params\n",
            "891.614   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e53b8f84aac43c398639381bf71ec10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 67230. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 56816. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "Global seed set to 42\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3a0ae8bf8794bb8a27dc4a1bacaba81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3db3e63ccd8f47739ab6f5371b64e9e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 70644. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 52483. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 14864. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 50481. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5382. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64467. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64536. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 31963. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 50814. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 55658. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 22202. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5099f6256a040d4aa7c9465f4870203",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea9df24dbb354845a752152ad4f4e64f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e51e9781a6ce48c69ec832ba7ee86a5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b72016de2276448686c346fc9991e580",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load (supports t5, mt5, byT5 models)\n",
        "model.from_pretrained(\"t5\",\"t5-base\")\n",
        "\n",
        "# train\n",
        "model.train(train_df=train_df, # pandas dataframe with 2 columns: source_text & target_text\n",
        "            eval_df=eval_df, # pandas dataframe with 2 columns: source_text & target_text\n",
        "            source_max_token_len = 512, \n",
        "            target_max_token_len = 128,\n",
        "            batch_size = 4,\n",
        "            max_epochs = 5,\n",
        "            use_gpu = True,\n",
        "            outputdir = \"outputs\",\n",
        "            early_stopping_patience_epochs = 0,\n",
        "            precision = 32\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsTc5Mn11qkT",
        "outputId": "35075ec8-bc35-4826-a6b2-56dd3c9a1493"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18688 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# load trained T5 model\n",
        "model.load_model(\"t5\",\"/content/outputs/simplet5-epoch-4-train-loss-2.5137/\", use_gpu=False)\n",
        "\n",
        "# predict\n",
        "lst_summ = []\n",
        "# for i in range(281, 286):\n",
        "#   lst_summ.append()\n",
        "print(model.predict(data_frame.iloc[281,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec2FQclcjvwz"
      },
      "outputs": [],
      "source": [
        "print(model.predict(source_text=data_frame.iloc[282,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwiYwh7fkJIa"
      },
      "outputs": [],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "od8usqY5NQ6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()"
      ],
      "metadata": {
        "id": "oKUxG9etNTrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.reset_index(drop=True, inplace =True)"
      ],
      "metadata": {
        "id": "Yxwz88krNVqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_summary = test_df.iloc[:,1]"
      ],
      "metadata": {
        "id": "gluRtcaCNYXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model for prediction\n",
        "model_out_put = []\n",
        "\n",
        "for i in range(test_df.shape[0]):\n",
        "  model_out_put.append(model.predict(\n",
        "        [\n",
        "           test_df.iloc[i,0]\n",
        "        ]\n",
        "    ))"
      ],
      "metadata": {
        "id": "Bp05sZqMNaOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the ROUGE value for all the test data\n",
        "import pandas as pd\n",
        "lst_of_df = []\n",
        "for i in range(len(reference_summary)):\n",
        "  model_out = model_out_put[i]\n",
        "  reference = reference_summary[i]\n",
        "  lst_of_df.append(pd.DataFrame(rouge.get_scores(model_out[0], reference)[0]))"
      ],
      "metadata": {
        "id": "zXDVG_ZiNc23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvwUWWYXUegg"
      },
      "outputs": [],
      "source": [
        "df1 = lst_of_df[0]\n",
        "df2 = lst_of_df[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZBUb9stUhll"
      },
      "outputs": [],
      "source": [
        "for i in range(2,len(lst_of_df)):\n",
        "  df2 = df2.add(df1, fill_value = 0)\n",
        "  df1 = lst_of_df[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uori4MwcUkc3"
      },
      "outputs": [],
      "source": [
        "# Average ROUGE score of the model over test data\n",
        "df2/test_df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ5QvXLPXaDC"
      },
      "outputs": [],
      "source": [
        "model_out_put[1]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38464ca0807c49388d06aa8860bce6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47afb74e6fbc48d6a77b2c031cbb6b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57e9672b3efa4818ab1ac8cd2a67384a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5cb6661963644bf082366b960385a7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba52d3fa74c148d596417390e9ba4ac7",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57e9672b3efa4818ab1ac8cd2a67384a",
            "value": 791656
          }
        },
        "86ef577dcccd43d4b5b308716fd6399a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05750c190784f798c422bad583e0515",
            "placeholder": "​",
            "style": "IPY_MODEL_38464ca0807c49388d06aa8860bce6a7",
            "value": "Downloading: 100%"
          }
        },
        "8856f77313c24a22925e551e6794a75e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a05750c190784f798c422bad583e0515": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b73f8c274aae4f57b657521f5a9fab90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8856f77313c24a22925e551e6794a75e",
            "placeholder": "​",
            "style": "IPY_MODEL_47afb74e6fbc48d6a77b2c031cbb6b75",
            "value": " 773k/773k [00:00&lt;00:00, 725kB/s]"
          }
        },
        "ba52d3fa74c148d596417390e9ba4ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d75db3dd436740e9b4670e5383fdf189": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce76502e0e94222a452690bc8b90c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86ef577dcccd43d4b5b308716fd6399a",
              "IPY_MODEL_5cb6661963644bf082366b960385a7e1",
              "IPY_MODEL_b73f8c274aae4f57b657521f5a9fab90"
            ],
            "layout": "IPY_MODEL_d75db3dd436740e9b4670e5383fdf189"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}